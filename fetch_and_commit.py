# fetch_and_commit.py

import os
import sys
import json
import requests
import gzip
import re
import argparse
from datetime import datetime, timezone
from pathlib import Path
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import partial

# --- –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ ---
try:
    from lxml import etree
except ImportError:
    sys.exit("–û—à–∏–±–∫–∞: lxml –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install lxml")
try:
    from thefuzz import fuzz
except ImportError:
    sys.exit("–û—à–∏–±–∫–∞: thefuzz –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install thefuzz python-Levenshtein")

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---
MAX_WORKERS = 100
SIMILARITY_THRESHOLD = 80
SOURCES_FILE = 'sources.json'
DATA_DIR = Path('data')
ICONS_DIR = Path('icons')
ICONS_MAP_FILE = Path('icons_map.json')
README_FILE = 'README.md'
RAW_BASE_URL = "https://raw.githubusercontent.com/{owner}/{repo}/main/{filepath}"

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---
def clean_name(name):
    name = name.lower()
    name = re.sub(r'\s*\b(hd|fhd|uhd|4k|8k|sd|low|vip|\(p\))\b', '', name, flags=re.IGNORECASE)
    name = re.sub(r'\[.*?\]|\(.*?\)', '', name)
    name = re.sub(r'[^\w\s]', '', name)
    return ' '.join(name.split())

def get_channel_names(channel_element):
    names = [el.text for el in channel_element.findall('display-name')]
    return {clean_name(name) for name in names if name}

def is_gzipped(file_path):
    with open(file_path, 'rb') as f:
        return f.read(2) == b'\x1f\x8b'

# --- –ö–ª–∞—Å—Å –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ---
class CustomEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, Path):
            return str(obj)
        if isinstance(obj, set):
            return list(obj)
        return json.JSONEncoder.default(self, obj)

# --- –û–°–ù–û–í–ù–´–ï –§–£–ù–ö–¶–ò–ò ---
def read_sources_and_notes():
    try:
        with open(SOURCES_FILE, 'r', encoding='utf-8') as f:
            config = json.load(f)
            sources, notes = config.get('sources', []), config.get('notes', '')
            if not sources: sys.exit("–û—à–∏–±–∫–∞: –≤ sources.json –Ω–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.")
            for s in sources: s.setdefault('ico_src', False)
            return sources, notes
    except Exception as e: sys.exit(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {SOURCES_FILE}: {e}")

def clear_data_dir():
    if DATA_DIR.exists():
        for f in DATA_DIR.iterdir():
            if f.is_file(): f.unlink()
    else:
        DATA_DIR.mkdir(parents=True, exist_ok=True)

def download_one(entry):
    url, desc = entry['url'], entry['desc']
    temp_path = DATA_DIR / ("tmp_" + os.urandom(4).hex())
    result = {'entry': entry, 'error': None}
    try:
        print(f"–ù–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É: {desc} ({url})")
        with requests.get(url, stream=True, timeout=120) as r:
            r.raise_for_status()
            with open(temp_path, 'wb') as f:
                for chunk in r.iter_content(16 * 1024): f.write(chunk)
        size_bytes = temp_path.stat().st_size
        size_mb = round(size_bytes / (1024 * 1024), 2)
        if size_bytes == 0: raise ValueError("–§–∞–π–ª –ø—É—Å—Ç–æ–π.")
        result.update({'size_mb': size_mb, 'temp_path': temp_path})
        return result
    except Exception as e:
        result['error'] = f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}"
        print(f"–û—à–∏–±–∫–∞ –¥–ª—è {desc}: {result['error']}")
        if temp_path.exists(): temp_path.unlink()
    return result

def download_icon(session, url, save_path):
    try:
        with session.get(url, stream=True, timeout=30) as r:
            r.raise_for_status()
            with open(save_path, 'wb') as f:
                for chunk in r.iter_content(8192): f.write(chunk)
        return True
    except requests.RequestException: return False

def _parse_icon_source_file(file_path, desc):
    found_icons = []
    try:
        open_func = gzip.open if is_gzipped(file_path) else open
        with open_func(file_path, 'rb') as f:
            tree = etree.parse(f)
        root = tree.getroot()
        for channel in root.findall('channel'):
            channel_id, icon_tag = channel.get('id'), channel.find('icon')
            if channel_id and icon_tag is not None and 'src' in icon_tag.attrib:
                icon_url, names = icon_tag.get('src'), get_channel_names(channel)
                if names and icon_url:
                    found_icons.append((desc, channel_id, names, icon_url))
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {file_path.name}: {e}", file=sys.stderr)
    return found_icons

def build_icon_database(download_results):
    print("\n--- –≠—Ç–∞–ø 1: –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏–∫–æ–Ω–æ–∫ ---")
    icon_db, icon_urls_to_download = {}, {}
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(_parse_icon_source_file, r['temp_path'], r['entry']['desc']) for r in download_results if not r.get('error') and r['entry']['ico_src']]
        for future in as_completed(futures):
            for desc, channel_id, names, icon_url in future.result():
                filename = Path(urlparse(icon_url).path).name or f"{channel_id}.png"
                local_icon_path = ICONS_DIR / filename
                db_key = f"{desc}_{channel_id}"
                icon_db[db_key] = {'icon_path': local_icon_path, 'names': names}
                icon_urls_to_download[icon_url] = local_icon_path
    
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(icon_db)} –∫–∞–Ω–∞–ª–æ–≤ —Å –∏–∫–æ–Ω–∫–∞–º–∏. –¢—Ä–µ–±—É–µ—Ç—Å—è —Å–∫–∞—á–∞—Ç—å {len(icon_urls_to_download)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–∫–æ–Ω–æ–∫.")
    adapter = requests.adapters.HTTPAdapter(pool_connections=MAX_WORKERS, pool_maxsize=MAX_WORKERS)
    with requests.Session() as session:
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            downloader = partial(download_icon, session)
            future_to_url = {executor.submit(downloader, url, path): url for url, path in icon_urls_to_download.items()}
            for future in as_completed(future_to_url): future.result()
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∫–æ–Ω–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
    return icon_db

def load_icon_map():
    print("\n--- –≠—Ç–∞–ø 1: –ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞—Ä—Ç—ã —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–∫–æ–Ω–æ–∫ ---")
    if not ICONS_MAP_FILE.is_file():
        print(f"–§–∞–π–ª {ICONS_MAP_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ö–∞—á–µ—Å—Ç–≤–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∏–∑–∫–∏–º.")
        print("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ (`--full-update`) –¥–ª—è –µ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è.")
        return {}
    with open(ICONS_MAP_FILE, 'r', encoding='utf-8') as f:
        icon_map_data = json.load(f)
    icon_db = {}
    for key, value in icon_map_data.items():
        icon_db[key] = {
            'icon_path': Path(value['icon_path']),
            'names': set(value['names'])
        }
    print(f"–ö–∞—Ä—Ç–∞ –∏–∫–æ–Ω–æ–∫ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ó–∞–ø–∏—Å–µ–π: {len(icon_db)}.")
    return icon_db

def find_best_match(channel_names, icon_db):
    if not channel_names: return None
    best_match_score = 0
    best_match_path = None
    for db_entry in icon_db.values():
        db_names = db_entry['names']
        if not db_names: continue
        if channel_names & db_names: return db_entry['icon_path']
        score = fuzz.token_set_ratio(' '.join(sorted(channel_names)), ' '.join(sorted(db_names)))
        if score > best_match_score:
            best_match_score = score
            best_match_path = db_entry['icon_path']
    if best_match_score >= SIMILARITY_THRESHOLD:
        return best_match_path
    return None

def process_epg_file(file_path, icon_db, owner, repo_name, entry):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω EPG-—Ñ–∞–π–ª: –Ω–∞—Ö–æ–¥–∏—Ç –∏ –∑–∞–º–µ–Ω—è–µ—Ç URL –∏–∫–æ–Ω–æ–∫."""
    print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–∞–π–ª: {file_path.name}")
    try:
        was_gzipped = is_gzipped(file_path)
        open_func = gzip.open if was_gzipped else open
        parser = etree.XMLParser(remove_blank_text=True)
        with open_func(file_path, 'rb') as f:
            tree = etree.parse(f, parser)
        root = tree.getroot()
        changes_made = 0
        for channel in root.findall('channel'):
            channel_names = get_channel_names(channel)
            matched_icon_path = find_best_match(channel_names, icon_db)
            if matched_icon_path:
                new_icon_url = RAW_BASE_URL.format(owner=owner, repo=repo_name, filepath=str(matched_icon_path))
                icon_tag = channel.find('icon')
                if icon_tag is None: icon_tag = etree.SubElement(channel, 'icon')
                if icon_tag.get('src') != new_icon_url:
                    icon_tag.set('src', new_icon_url)
                    changes_made += 1
        
        if changes_made > 0:
            print(f"–í–Ω–µ—Å–µ–Ω–æ {changes_made} –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –∏–∫–æ–Ω–∫–∏ —Ñ–∞–π–ª–∞ {file_path.name}.")
            doctype_str = '<!DOCTYPE tv SYSTEM "https://iptvx.one/xmltv.dtd">'
            xml_bytes = etree.tostring(tree, pretty_print=True, xml_declaration=True, encoding='UTF-8', doctype=doctype_str)
            
            if was_gzipped:
                # <<< –ò–ó–ú–ï–ù–ï–ù–ò–ï –ó–î–ï–°–¨ >>>
                original_filename = Path(urlparse(entry['url']).path).name
                # –ï—Å–ª–∏ —É –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –µ—Å—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ .gz, —É–±–∏—Ä–∞–µ–º –µ–≥–æ
                if original_filename.lower().endswith('.gz'):
                    archive_internal_name = original_filename[:-3]
                # –ò–Ω–∞—á–µ (–µ—Å–ª–∏ —ç—Ç–æ EPG_LITE), –¥–æ–±–∞–≤–ª—è–µ–º .xml
                else:
                    archive_internal_name = f"{original_filename}.xml"
                # <<< –ö–û–ù–ï–¶ –ò–ó–ú–ï–ù–ï–ù–ò–Ø >>>
                
                with gzip.GzipFile(filename=archive_internal_name, mode='wb', fileobj=open(file_path, 'wb'), mtime=0) as f_out:
                    f_out.write(xml_bytes)
            else:
                with open(file_path, 'wb') as f_out: f_out.write(xml_bytes)
        return True
    except Exception as e:
        print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path}: {e}", file=sys.stderr)
        return False

def update_readme(results, notes):
    utc_now, timestamp = datetime.now(timezone.utc), datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M %Z')
    lines = [notes, "\n---"] if notes else []
    lines.append(f"\n# –û–±–Ω–æ–≤–ª–µ–Ω–æ: {timestamp}\n")
    for idx, r in enumerate(results, 1):
        lines.append(f"### {idx}. {r['entry']['desc']}\n")
        if r.get('error'):
            lines.extend([f"**–°—Ç–∞—Ç—É—Å:** üî¥ –û—à–∏–±–∫–∞", f"**–ò—Å—Ç–æ—á–Ω–∏–∫:** `{r['entry']['url']}`", f"**–ü—Ä–∏—á–∏–Ω–∞:** {r.get('error')}"])
        else:
            lines.extend([f"**–†–∞–∑–º–µ—Ä:** {r['size_mb']} MB", "", f"**–°—Å—ã–ª–∫–∞ –¥–ª—è –ø–ª–µ–µ—Ä–∞ (GitHub Raw):**", f"`{r['raw_url']}`"])
        lines.append("\n---")
    with open(README_FILE, 'w', encoding='utf-8') as f:
        f.write("\n".join(lines))
    print(f"README.md –æ–±–Ω–æ–≤–ª—ë–Ω ({len(results)} –∑–∞–ø–∏—Å–µ–π)")

def main():
    parser = argparse.ArgumentParser(description="EPG Updater Script")
    parser.add_argument('--full-update', action='store_true', help='Perform a full update, including icons.')
    args = parser.parse_args()

    repo = os.getenv('GITHUB_REPOSITORY')
    if not repo or '/' not in repo: sys.exit("–û—à–∏–±–∫–∞: GITHUB_REPOSITORY –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω.")
    
    owner, repo_name = repo.split('/')
    sources, notes = read_sources_and_notes()
    clear_data_dir()

    print("\n--- –≠—Ç–∞–ø 0: –ó–∞–≥—Ä—É–∑–∫–∞ EPG —Ñ–∞–π–ª–æ–≤ ---")
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        download_results = list(executor.map(download_one, sources))

    if args.full_update:
        print("\n–ó–∞–ø—É—â–µ–Ω —Ä–µ–∂–∏–º –ü–û–õ–ù–û–ì–û –û–ë–ù–û–í–õ–ï–ù–ò–Ø.")
        if ICONS_DIR.exists():
            for f in ICONS_DIR.iterdir():
                if f.is_file(): f.unlink()
        else:
            ICONS_DIR.mkdir(parents=True, exist_ok=True)
        
        icon_db = build_icon_database(download_results)
        
        print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞—Ä—Ç—ã –∏–∫–æ–Ω–æ–∫ –≤ {ICONS_MAP_FILE}...")
        with open(ICONS_MAP_FILE, 'w', encoding='utf-8') as f:
            json.dump(icon_db, f, ensure_ascii=False, indent=2, cls=CustomEncoder)
        print("–ö–∞—Ä—Ç–∞ –∏–∫–æ–Ω–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")

    else:
        print("\n–ó–∞–ø—É—â–µ–Ω —Ä–µ–∂–∏–º –ï–ñ–ï–î–ù–ï–í–ù–û–ì–û –û–ë–ù–û–í–õ–ï–ù–ò–Ø.")
        icon_db = load_icon_map()

    print("\n--- –≠—Ç–∞–ø 2: –ó–∞–º–µ–Ω–∞ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏–∫–æ–Ω–∫–∏ –≤ EPG —Ñ–∞–π–ª–∞—Ö ---")
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(process_epg_file, res['temp_path'], icon_db, owner, repo_name, res['entry']) for res in download_results if not res.get('error')]
        for future in as_completed(futures): future.result()
    
    print("\n--- –≠—Ç–∞–ø 3: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∏ README ---")
    url_to_result = {res['entry']['url']: res for res in download_results}
    ordered_results = [url_to_result[s['url']] for s in sources]
    final_results, used_names = [], set()

    for res in ordered_results:
        if res.get('error'):
            final_results.append(res)
            continue
        final_filename_from_url = Path(urlparse(res['entry']['url']).path).name
        if not Path(final_filename_from_url).suffix:
            proposed_filename = f"{final_filename_from_url}{'.xml.gz' if is_gzipped(res['temp_path']) else '.xml'}"
        else:
            proposed_filename = final_filename_from_url
        final_name, counter = proposed_filename, 1
        while final_name in used_names:
            p, stem = Path(proposed_filename), p.name.replace(''.join(p.suffixes), '')
            final_name = f"{stem}-{counter}{''.join(p.suffixes)}"
            counter += 1
        used_names.add(final_name)
        target_path = DATA_DIR / final_name
        res['temp_path'].rename(target_path)
        res['raw_url'] = RAW_BASE_URL.format(owner=owner, repo=repo_name, filepath=str(target_path))
        final_results.append(res)

    update_readme(final_results, notes)
    print("\n–°–∫—Ä–∏–ø—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É.")

if __name__ == '__main__':
    main()
