import os
import sys
import json
import requests
from datetime import datetime, timezone
from pathlib import Path
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import gdshortener
import gzip
import re
from difflib import SequenceMatcher

try:
    from lxml import etree
except ImportError:
    print("–û—à–∏–±–∫–∞: –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ lxml –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ–µ: pip install lxml", file=sys.stderr)
    sys.exit(1)

SOURCES_FILE = 'sources.json'
DATA_DIR = Path('data')
ICONS_DIR = Path('icons') 
README_FILE = 'README.md'
MAX_WORKERS = 10
CHUNK_SIZE = 16 * 1024
MAX_FILE_SIZE_MB = 95
JSDELIVR_SIZE_LIMIT_MB = 20
SIMILARITY_THRESHOLD = 0.8 

RAW_BASE_URL = "https://raw.githubusercontent.com/{owner}/{repo}/main/{filepath}"
JSDELIVR_BASE_URL = "https://cdn.jsdelivr.net/gh/{owner}/{repo}@main/{filepath}"

def is_gzipped(file_path):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª gzipped, –ø–æ –µ–≥–æ –º–∞–≥–∏—á–µ—Å–∫–∏–º –±–∞–π—Ç–∞–º."""
    with open(file_path, 'rb') as f:
        return f.read(2) == b'\x1f\x8b'

def clean_name(name):
    """–û—á–∏—â–∞–µ—Ç –∏–º—è –∫–∞–Ω–∞–ª–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è."""
    name = name.lower()
    name = re.sub(r'\s*\b(hd|fhd|uhd|4k|8k|sd|low|vip|\(p\))\b', '', name, flags=re.IGNORECASE)
    name = re.sub(r'\[.*?\]|\(.*?\)', '', name)
    name = re.sub(r'[^\w\s]', '', name)
    return ' '.join(name.split())

def get_channel_names(channel_element):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –æ—á–∏—â–∞–µ—Ç –≤—Å–µ display-name –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞ channel."""
    names = [el.text for el in channel_element.findall('display-name')]
    return {clean_name(name) for name in names if name}



def read_sources_and_notes():
    try:
        with open(SOURCES_FILE, 'r', encoding='utf-8') as f:
            config = json.load(f)
            sources = config.get('sources', [])
            notes = config.get('notes', '')
            if not sources:
                print("–û—à–∏–±–∫–∞: –≤ sources.json –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –≤ –∫–ª—é—á–µ 'sources'.", file=sys.stderr)
                sys.exit(1)
            for s in sources:
                s.setdefault('ico_src', False)
            return sources, notes
    except FileNotFoundError:
        print(f"–û—à–∏–±–∫–∞: –§–∞–π–ª {SOURCES_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω.", file=sys.stderr)
        sys.exit(1)
    except json.JSONDecodeError:
        print(f"–û—à–∏–±–∫–∞: –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç JSON –≤ —Ñ–∞–π–ª–µ {SOURCES_FILE}.", file=sys.stderr)
        sys.exit(1)

def clear_dirs():
    """–û—á–∏—â–∞–µ—Ç –ø–∞–ø–∫–∏ data –∏ icons."""
    for d in [DATA_DIR, ICONS_DIR]:
        if d.exists():
            for f in d.iterdir():
                if f.is_file():
                    f.unlink()
        else:
            d.mkdir(parents=True, exist_ok=True)
    gitignore = ICONS_DIR / '.gitignore'
    if not gitignore.exists():
        gitignore.write_text('*\n!.gitignore')


def download_one(entry):
    url = entry['url']
    desc = entry['desc']
    temp_path = DATA_DIR / ("tmp_" + os.urandom(4).hex())
    result = {'entry': entry, 'error': None}
    try:
        print(f"–ù–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É: {desc} ({url})")
        with requests.get(url, stream=True, timeout=120) as r:
            r.raise_for_status()
            with open(temp_path, 'wb') as f:
                for chunk in r.iter_content(CHUNK_SIZE):
                    f.write(chunk)
        size_bytes = temp_path.stat().st_size
        size_mb = round(size_bytes / (1024 * 1024), 2)
        if size_bytes == 0:
            raise ValueError("–§–∞–π–ª –ø—É—Å—Ç–æ–π.")
        if size_bytes > MAX_FILE_SIZE_MB * 1024 * 1024:
            raise ValueError(f"–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({size_mb} MB > {MAX_FILE_SIZE_MB} MB).")
        
        result.update({
            'size_mb': size_mb,
            'temp_path': temp_path
        })
        return result
    except Exception as e:
        result['error'] = f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}"
        print(f"–û—à–∏–±–∫–∞ –¥–ª—è {desc}: {result['error']}")
        if temp_path.exists():
            temp_path.unlink()
    return result


def download_icon(url, save_path):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –æ–¥–Ω—É –∏–∫–æ–Ω–∫—É."""
    try:
        with requests.get(url, stream=True, timeout=30) as r:
            r.raise_for_status()
            with open(save_path, 'wb') as f:
                for chunk in r.iter_content(8192):
                    f.write(chunk)
        return True
    except requests.RequestException as e:
        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –∏–∫–æ–Ω–∫—É {url}: {e}", file=sys.stderr)
        return False

def build_icon_database(download_results):
    """–°–∫–∞–Ω–∏—Ä—É–µ—Ç EPG-–∏—Å—Ç–æ—á–Ω–∏–∫–∏, –ø–æ–º–µ—á–µ–Ω–Ω—ã–µ –∫–∞–∫ ico_src, —Å–∫–∞—á–∏–≤–∞–µ—Ç –∏–∫–æ–Ω–∫–∏ –∏ —Å–æ–∑–¥–∞–µ—Ç –±–∞–∑—É."""
    print("\n--- –≠—Ç–∞–ø 1: –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏–∫–æ–Ω–æ–∫ ---")
    icon_db = {}
    icon_urls_to_download = {}

    for result in download_results:
        if result.get('error') or not result['entry']['ico_src']:
            continue
        
        print(f"–°–∫–∞–Ω–∏—Ä—É—é –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–∫–æ–Ω–æ–∫: {result['entry']['desc']}")
        file_path = result['temp_path']
        
        try:
            # <<< –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é is_gzipped –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–∞ –æ—Ç–∫—Ä—ã—Ç–∏—è >>>
            open_func = gzip.open if is_gzipped(file_path) else open
            with open_func(file_path, 'rb') as f:
                # lxml.etree.parse –º–æ–∂–µ—Ç –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ñ–∞–π–ª–æ–≤—ã–π –æ–±—ä–µ–∫—Ç
                tree = etree.parse(f)
            root = tree.getroot()

            for channel in root.findall('channel'):
                channel_id = channel.get('id')
                icon_tag = channel.find('icon')
                if channel_id and icon_tag is not None and 'src' in icon_tag.attrib:
                    icon_url = icon_tag.get('src')
                    names = get_channel_names(channel)
                    if not names or not icon_url:
                        continue

                    parsed_url = urlparse(icon_url)
                    filename = Path(parsed_url.path).name or f"{channel_id}.png"
                    
                    local_icon_path = ICONS_DIR / filename
                    
                    db_key = f"{result['entry']['desc']}_{channel_id}"
                    icon_db[db_key] = {'icon_path': local_icon_path, 'names': names}
                    
                    if not local_icon_path.exists():
                        icon_urls_to_download[icon_url] = local_icon_path

        except (etree.XMLSyntaxError, gzip.BadGzipFile, ValueError) as e:
            # <<< –ò–ó–ú–ï–ù–ï–ù–ò–ï: –î–æ–±–∞–≤–∏–ª–∏ –∏–º—è —Ñ–∞–π–ª–∞ –≤ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏ >>>
            print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {file_path.name} –¥–ª—è —Å–±–æ—Ä–∞ –∏–∫–æ–Ω–æ–∫: {e}", file=sys.stderr)

    print(f"–ù–∞–π–¥–µ–Ω–æ {len(icon_db)} –∫–∞–Ω–∞–ª–æ–≤ —Å –∏–∫–æ–Ω–∫–∞–º–∏ –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö.")
    print(f"–¢—Ä–µ–±—É–µ—Ç—Å—è —Å–∫–∞—á–∞—Ç—å {len(icon_urls_to_download)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–∫–æ–Ω–æ–∫.")
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS * 2) as executor:
        future_to_url = {executor.submit(download_icon, url, path): url for url, path in icon_urls_to_download.items()}
        for future in as_completed(future_to_url):
            future.result() 

    print("–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∫–æ–Ω–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
    return icon_db


def find_best_match(channel_names, icon_db):
    """–ù–∞—Ö–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â—É—é –∏–∫–æ–Ω–∫—É –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö."""
    if not channel_names:
        return None
        
    best_match_score = 0
    best_match_path = None

    for db_entry in icon_db.values():
        db_names = db_entry['names']
        if not db_names:
            continue
        
        if channel_names & db_names:
            return db_entry['icon_path']

        current_max_score = 0
        for name1 in channel_names:
            for name2 in db_names:
                score = SequenceMatcher(None, name1, name2).ratio()
                if score > current_max_score:
                    current_max_score = score
        
        if current_max_score > best_match_score:
            best_match_score = current_max_score
            best_match_path = db_entry['icon_path']
            
    if best_match_score >= SIMILARITY_THRESHOLD:
        return best_match_path
        
    return None

def process_epg_file(file_path, icon_db, owner, repo_name):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω EPG —Ñ–∞–π–ª: –Ω–∞—Ö–æ–¥–∏—Ç –∏ –∑–∞–º–µ–Ω—è–µ—Ç URL –∏–∫–æ–Ω–æ–∫."""
    print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–∞–π–ª: {file_path.name}")
    try:
        # <<< –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ó–∞–ø–æ–º–∏–Ω–∞–µ–º, –±—ã–ª –ª–∏ —Ñ–∞–π–ª —Å–∂–∞—Ç –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ >>>
        was_gzipped = is_gzipped(file_path)
        open_func = gzip.open if was_gzipped else open
        
        parser = etree.XMLParser(remove_blank_text=True)
        with open_func(file_path, 'rb') as f:
            tree = etree.parse(f, parser)
        root = tree.getroot()
        
        changes_made = 0
        for channel in root.findall('channel'):
            channel_names = get_channel_names(channel)
            matched_icon_path = find_best_match(channel_names, icon_db)

            if matched_icon_path:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –¥–ª—è —Å—Å—ã–ª–æ–∫
                new_icon_url = RAW_BASE_URL.format(owner=owner, repo=repo_name, filepath=matched_icon_path.as_posix())
                
                icon_tag = channel.find('icon')
                if icon_tag is None:
                    icon_tag = etree.SubElement(channel, 'icon')
                
                if icon_tag.get('src') != new_icon_url:
                    icon_tag.set('src', new_icon_url)
                    changes_made += 1
        
        if changes_made > 0:
            print(f"–í–Ω–µ—Å–µ–Ω–æ {changes_made} –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –∏–∫–æ–Ω–∫–∏ —Ñ–∞–π–ª–∞ {file_path.name}.")
            doctype_str = '<!DOCTYPE tv SYSTEM "https://iptvx.one/xmltv.dtd">'
            
            # <<< –ò–ó–ú–ï–ù–ï–ù–ò–ï: –õ–æ–≥–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ–ø–µ—Ä—å —Ç–æ–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç was_gzipped >>>
            if was_gzipped:
                # –°—Ä–∞–∑—É –ø–∏—à–µ–º –≤ —Å–∂–∞—Ç—ã–π —Ñ–∞–π–ª
                with gzip.open(file_path, 'wb') as f_out:
                    f_out.write(etree.tostring(tree, pretty_print=True, xml_declaration=True, encoding='UTF-8', doctype=doctype_str))
            else:
                # –ü–∏—à–µ–º –≤ –æ–±—ã—á–Ω—ã–π —Ñ–∞–π–ª
                tree.write(str(file_path), pretty_print=True, xml_declaration=True, encoding='UTF-8', doctype=doctype_str)

        return True

    except Exception as e:
        print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path}: {e}", file=sys.stderr)
        return False


def shorten_url_safely(url):
    try:
        shortener = gdshortener.ISGDShortener()
        short_tuple = shortener.shorten(url)
        return short_tuple[0] if short_tuple and short_tuple[0] else "–Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∫—Ä–∞—Ç–∏—Ç—å"
    except Exception as e:
        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∫—Ä–∞—Ç–∏—Ç—å URL {url}: {e}", file=sys.stderr)
        return "–Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∫—Ä–∞—Ç–∏—Ç—å"


def update_readme(results, notes):
    utc_now = datetime.now(timezone.utc)
    timestamp = utc_now.strftime('%Y-%m-%d %H:%M %Z')
    
    lines = []

    if notes:
        lines.append(notes)
        lines.append("\n---")
    
    lines.append(f"\n# –û–±–Ω–æ–≤–ª–µ–Ω–æ: {timestamp}\n")

    for idx, r in enumerate(results, 1):
        lines.append(f"### {idx}. {r['entry']['desc']}")
        lines.append("")
        if r.get('error'):
            lines.append(f"**–°—Ç–∞—Ç—É—Å:** üî¥ –û—à–∏–±–∫–∞")
            lines.append(f"**–ò—Å—Ç–æ—á–Ω–∏–∫:** `{r['entry']['url']}`")
            lines.append(f"**–ü—Ä–∏—á–∏–Ω–∞:** {r.get('error')}")
        else:
            lines.append(f"**–†–∞–∑–º–µ—Ä:** {r['size_mb']} MB")
            lines.append("")
            
            lines.append(f"**–û—Å–Ω–æ–≤–Ω–∞—è —Å—Å—ã–ª–∫–∞ (GitHub Raw):**")
            lines.append(f"`{r['raw_url']}`")
            lines.append("")

            lines.append("> **–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å—Å—ã–ª–∫–∏:**")
            lines.append(">") 
            lines.append(f"> - *–ö–æ—Ä–æ—Ç–∫–∞—è (–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø–ª–µ–µ—Ä—ã –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç):* `{r['short_raw_url']}`")
            
            if r.get('jsdelivr_url'):
                lines.append(f"> - *CDN (jsDelivr):* `{r['jsdelivr_url']}` (–ö–æ—Ä–æ—Ç–∫–∞—è (–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø–ª–µ–µ—Ä—ã –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç): `{r['short_jsdelivr_url']}`)")

        lines.append("\n---")

    with open(README_FILE, 'w', encoding='utf-8') as f:
        f.write("\n".join(lines))
    print(f"README.md –æ–±–Ω–æ–≤–ª—ë–Ω ({len(results)} –∑–∞–ø–∏—Å–µ–π)")


def main():
    repo = os.getenv('GITHUB_REPOSITORY')
    if not repo or '/' not in repo:
        print("–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å GITHUB_REPOSITORY.", file=sys.stderr)
        sys.exit(1)
    
    owner, repo_name = repo.split('/')
    
    sources, notes = read_sources_and_notes()
    clear_dirs()

    print("\n--- –≠—Ç–∞–ø 0: –ó–∞–≥—Ä—É–∑–∫–∞ EPG —Ñ–∞–π–ª–æ–≤ ---")
    download_results = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_entry = {executor.submit(download_one, entry): entry for entry in sources}
        for future in as_completed(future_to_entry):
            download_results.append(future.result())

    icon_db = build_icon_database(download_results)
    
    print("\n--- –≠—Ç–∞–ø 2: –ó–∞–º–µ–Ω–∞ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏–∫–æ–Ω–∫–∏ –≤ EPG —Ñ–∞–π–ª–∞—Ö ---")
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = []
        for res in download_results:
            if not res.get('error'):
                futures.append(executor.submit(process_epg_file, res['temp_path'], icon_db, owner, repo_name))
        for future in as_completed(futures):
            future.result() 
    
    # --- –≠—Ç–∞–ø 3: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏–µ README ---
    print("\n--- –≠—Ç–∞–ø 3: –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –∏ README.md ---")
    
    url_to_result = {res['entry']['url']: res for res in download_results}
    ordered_results = [url_to_result[s['url']] for s in sources]

    final_results = []
    used_names = set()
    for res in ordered_results:
        if res.get('error'):
            final_results.append(res)
            continue
        
        # <<< –ò–ó–ú–ï–ù–ï–ù–ò–ï: –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞–¥–µ–∂–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–∞ >>>
        if is_gzipped(res['temp_path']):
            true_extension = '.xml.gz'
        else:
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ XML –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
            try:
                with open(res['temp_path'], 'rb') as f:
                    sig = f.read(5)
                if sig.startswith(b'<?xml'):
                    true_extension = '.xml'
                else:
                    # –ï—Å–ª–∏ –Ω–µ gzip –∏ –Ω–µ xml, –±–µ—Ä–µ–º –∏–∑ URL
                    true_extension = ''.join(Path(urlparse(res['entry']['url']).path).suffixes) or '.xml'
            except Exception:
                true_extension = '.xml'
             
        filename_from_url = Path(urlparse(res['entry']['url']).path).name or "download"
        base_name = filename_from_url.split('.')[0]
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º .suffixes –¥–ª—è —Å–ª—É—á–∞–µ–≤ —Ç–∏–ø–∞ file.xml.gz
        proposed_filename = f"{base_name}{true_extension}"

        final_name = proposed_filename
        counter = 1
        while final_name in used_names:
            p = Path(proposed_filename)
            # –ü—Ä–∞–≤–∏–º —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–º–µ–Ω–∏ –¥–ª—è —Å–ª—É—á–∞–µ–≤ —Å –¥–≤–æ–π–Ω—ã–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º
            stem = p.name.replace(''.join(p.suffixes), '')
            final_name = f"{stem}-{counter}{''.join(p.suffixes)}"
            counter += 1
        
        used_names.add(final_name)
        
        target_path = DATA_DIR / final_name
        res['temp_path'].rename(target_path)
        
        raw_url = RAW_BASE_URL.format(owner=owner, repo=repo_name, filepath=target_path.as_posix())
        res['raw_url'] = raw_url
        res['short_raw_url'] = shorten_url_safely(raw_url)
        
        if res['size_mb'] < JSDELIVR_SIZE_LIMIT_MB:
            jsdelivr_url = JSDELIVR_BASE_URL.format(owner=owner, repo=repo_name, filepath=target_path.as_posix())
            res['jsdelivr_url'] = jsdelivr_url
            res['short_jsdelivr_url'] = shorten_url_safely(jsdelivr_url)
        
        final_results.append(res)

    update_readme(final_results, notes)
    print("\n–°–∫—Ä–∏–ø—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É.")

if __name__ == '__main__':
    main()
