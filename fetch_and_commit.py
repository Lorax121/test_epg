import os
import sys
import json
import requests
import gzip
import re
import argparse
import hashlib
import time
import signal
from datetime import datetime, timezone
from pathlib import Path
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError
from functools import partial
from collections import defaultdict
from contextlib import contextmanager

try:
    from lxml import etree
except ImportError:
    sys.exit("–û—à–∏–±–∫–∞: lxml –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install lxml")

# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è GitHub Actions
MAX_WORKERS = 20  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
ICON_DOWNLOAD_WORKERS = 10  # –û—Ç–¥–µ–ª—å–Ω—ã–π –ª–∏–º–∏—Ç –¥–ª—è –∏–∫–æ–Ω–æ–∫
REQUEST_TIMEOUT = 45  # –£–≤–µ–ª–∏—á–µ–Ω timeout –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
ICON_TIMEOUT = 15  # –ö–æ—Ä–æ—Ç–∫–∏–π timeout –¥–ª—è –∏–∫–æ–Ω–æ–∫
MAX_ICON_DOWNLOAD_TIME = 1200  # –ú–∞–∫—Å–∏–º—É–º 20 –º–∏–Ω—É—Ç –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É –≤—Å–µ—Ö –∏–∫–æ–Ω–æ–∫

SOURCES_FILE = 'sources.json'
DATA_DIR = Path('data')
ICONS_DIR = Path('icons')
ICONS_MAP_FILE = Path('icons_map.json')
README_FILE = 'README.md'
RAW_BASE_URL = "https://raw.githubusercontent.com/{owner}/{repo}/main/{filepath}"

class TimeoutHandler:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–∞–π–º–∞—É—Ç–æ–≤ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∑–∞–≤–∏—Å–∞–Ω–∏—è"""
    def __init__(self):
        self.timeout_occurred = False
    
    def handler(self, signum, frame):
        self.timeout_occurred = True
        print(f"\n‚ö†Ô∏è  –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª —Ç–∞–π–º–∞—É—Ç–∞ ({signum}). –ó–∞–≤–µ—Ä—à–∞—é —Ç–µ–∫—É—â—É—é –æ–ø–µ—Ä–∞—Ü–∏—é...")

timeout_handler = TimeoutHandler()

@contextmanager
def timeout_context(seconds):
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç-–º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ —Ç–∞–π–º–∞—É—Ç–∞ –Ω–∞ –æ–ø–µ—Ä–∞—Ü–∏—é"""
    old_handler = signal.signal(signal.SIGALRM, timeout_handler.handler)
    signal.alarm(seconds)
    try:
        yield
    finally:
        signal.alarm(0)
        signal.signal(signal.SIGALRM, old_handler)

def is_gzipped(file_path):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª gzipped."""
    try:
        with open(file_path, 'rb') as f:
            return f.read(2) == b'\x1f\x8b'
    except:
        return False

class CustomEncoder(json.JSONEncoder):
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Path –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –º–Ω–æ–∂–µ—Å—Ç–≤ –≤ JSON."""
    def default(self, obj):
        if isinstance(obj, Path):
            return str(obj).replace('\\', '/') 
        if isinstance(obj, set):
            return list(obj)
        return json.JSONEncoder.default(self, obj)

def read_sources_and_notes():
    """–ß–∏—Ç–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ –∑–∞–º–µ—Ç–∫–∏ –∏–∑ sources.json."""
    try:
        with open(SOURCES_FILE, 'r', encoding='utf-8') as f:
            config = json.load(f)
            sources, notes = config.get('sources', []), config.get('notes', '')
            if not sources:
                sys.exit("–û—à–∏–±–∫–∞: –≤ sources.json –Ω–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.")
            return sources, notes
    except Exception as e:
        sys.exit(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {SOURCES_FILE}: {e}")

def clear_directory(dir_path: Path):
    """–û—á–∏—â–∞–µ—Ç —É–∫–∞–∑–∞–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –æ—Ç —Ñ–∞–π–ª–æ–≤ –∏ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π."""
    if dir_path.exists():
        for item in dir_path.iterdir():
            try:
                if item.is_dir():
                    clear_directory(item)
                    item.rmdir()
                else:
                    item.unlink()
            except Exception as e:
                print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {item}: {e}")
    else:
        dir_path.mkdir(parents=True, exist_ok=True)

def download_one(entry):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –æ–¥–∏–Ω EPG —Ñ–∞–π–ª —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞."""
    url, desc = entry['url'], entry['desc']
    temp_path = DATA_DIR / ("tmp_" + os.urandom(4).hex())
    result = {'entry': entry, 'error': None}
    
    try:
        print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞—é: {desc}")
        print(f"   URL: {url}")
        
        with requests.get(url, stream=True, timeout=REQUEST_TIMEOUT) as r:
            r.raise_for_status()
            total_size = int(r.headers.get('content-length', 0))
            downloaded = 0
            
            with open(temp_path, 'wb') as f:
                last_progress_time = time.time()
                for chunk in r.iter_content(32 * 1024):  # –£–≤–µ–ª–∏—á–µ–Ω —Ä–∞–∑–º–µ—Ä chunk
                    if timeout_handler.timeout_occurred:
                        raise TimeoutError("–û–ø–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ —Ç–∞–π–º–∞—É—Ç—É")
                    
                    f.write(chunk)
                    downloaded += len(chunk)
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 5 —Å–µ–∫—É–Ω–¥
                    current_time = time.time()
                    if current_time - last_progress_time >= 5:
                        if total_size > 0:
                            progress = (downloaded / total_size) * 100
                            print(f"   –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}% ({downloaded / (1024*1024):.1f} MB / {total_size / (1024*1024):.1f} MB)")
                        else:
                            print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {downloaded / (1024*1024):.1f} MB")
                        last_progress_time = current_time
        
        size_bytes = temp_path.stat().st_size
        if size_bytes == 0:
            raise ValueError("–§–∞–π–ª –ø—É—Å—Ç–æ–π.")
            
        size_mb = round(size_bytes / (1024 * 1024), 2)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {desc} ({size_mb} MB)")
        
        result.update({'size_mb': size_mb, 'temp_path': temp_path})
        return result
        
    except Exception as e:
        result['error'] = f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}"
        print(f"‚ùå –û—à–∏–±–∫–∞ –¥–ª—è {desc}: {result['error']}")
        if temp_path.exists():
            temp_path.unlink()
    return result

def download_icon_batch(session, batch_items):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –ø–∞–∫–µ—Ç –∏–∫–æ–Ω–æ–∫ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º."""
    successful = 0
    for url, save_path in batch_items:
        if timeout_handler.timeout_occurred:
            break
        try:
            save_path.parent.mkdir(parents=True, exist_ok=True)
            with session.get(url, stream=True, timeout=ICON_TIMEOUT) as r:
                r.raise_for_status()
                with open(save_path, 'wb') as f:
                    for chunk in r.iter_content(8192):
                        f.write(chunk)
            successful += 1
        except:
            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∏–∫–æ–Ω–æ–∫
            continue
    return successful

def get_icon_signature_fast(file_path):
    """–ë—ã—Å—Ç—Ä–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Å–∏–≥–Ω–∞—Ç—É—Ä—ã EPG-—Ñ–∞–π–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ URL-–æ–≤ –∏–∫–æ–Ω–æ–∫."""
    icon_urls = set()
    try:
        open_func = gzip.open if is_gzipped(file_path) else open
        with open_func(file_path, 'rb') as f:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º
            context = etree.iterparse(f, tag='icon', events=('end',))
            count = 0
            for _, element in context:
                if count > 10000:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
                    break
                if 'src' in element.attrib:
                    icon_urls.add(element.attrib['src'])
                element.clear()
                count += 1
                
                # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ç–∞–π–º–∞—É—Ç
                if count % 1000 == 0 and timeout_handler.timeout_occurred:
                    break
        
        if not icon_urls:
            return None
            
        sorted_urls = sorted(list(icon_urls))
        return hashlib.sha256(''.join(sorted_urls).encode('utf-8')).hexdigest()
        
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Å–∏–≥–Ω–∞—Ç—É—Ä—ã –¥–ª—è {file_path.name}: {e}")
        return None

def perform_full_update(download_results):
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π –∏–∫–æ–Ω–æ–∫."""
    print("\n--- –≠—Ç–∞–ø 1: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ –Ω–∞–±–æ—Ä–∞–º –∏–∫–æ–Ω–æ–∫ ---")
    groups = defaultdict(list)
    
    for i, res in enumerate(download_results):
        if res.get('error'):
            continue
            
        print(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ñ–∞–π–ª {i+1}/{len(download_results)}: {res['entry']['desc']}")
        signature = get_icon_signature_fast(res['temp_path'])
        groups[signature].append(res)
        
        if timeout_handler.timeout_occurred:
            print("‚ö†Ô∏è  –û–ø–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ —Ç–∞–π–º–∞—É—Ç—É –Ω–∞ —ç—Ç–∞–ø–µ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏")
            break
    
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(groups)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≥—Ä—É–ø–ø –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.")

    icon_data = {
        "icon_pool": {},
        "groups": {},
        "source_to_group": {}
    }
    all_unique_urls = set()

    print("\n--- –≠—Ç–∞–ø 2: –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Ä—Ç –∏–∫–æ–Ω–æ–∫ ---")
    for i, (signature, sources_in_group) in enumerate(groups.items()):
        if timeout_handler.timeout_occurred:
            break
            
        print(f"üìã –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –≥—Ä—É–ø–ø—É {i+1}/{len(groups)}")
        
        if signature is None:
            for res in sources_in_group:
                icon_data["source_to_group"][res['entry']['url']] = None
            continue
        
        icon_map_for_group = {}
        representative_file = sources_in_group[0]['temp_path']
        
        try:
            open_func = gzip.open if is_gzipped(representative_file) else open
            with open_func(representative_file, 'rb') as f:
                context = etree.iterparse(f, tag='channel', events=('end',))
                count = 0
                for _, channel in context:
                    if count > 5000 or timeout_handler.timeout_occurred:
                        break
                    channel_id = channel.get('id')
                    icon_tag = channel.find('icon')
                    if channel_id and icon_tag is not None and 'src' in icon_tag.attrib:
                        icon_url = icon_tag.get('src')
                        icon_map_for_group[channel_id] = icon_url
                        all_unique_urls.add(icon_url)
                    channel.clear()
                    count += 1
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ñ–∞–π–ª–∞ {representative_file.name}: {e}")
            continue

        icon_data["groups"][signature] = {"icon_map": icon_map_for_group}
        for res in sources_in_group:
            icon_data["source_to_group"][res['entry']['url']] = signature
        
        print(f"   ‚úÖ –ì—Ä—É–ø–ø–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞: {len(icon_map_for_group)} –∏–∫–æ–Ω–æ–∫")

    # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∫–æ–Ω–æ–∫
    icon_pool_dir = ICONS_DIR / "pool"
    urls_to_download = {}
    
    print(f"\n--- –≠—Ç–∞–ø 2.1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {len(all_unique_urls)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–∫–æ–Ω–æ–∫ ---")
    
    for url in all_unique_urls:
        url_hash = hashlib.sha1(url.encode('utf-8')).hexdigest()
        original_ext = "".join(Path(urlparse(url).path).suffixes) if Path(urlparse(url).path).suffixes else ".png"
        pool_path = icon_pool_dir / f"{url_hash}{original_ext}"
        
        icon_data["icon_pool"][url] = pool_path
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ —Å–∫–∞—á–∏–≤–∞—Ç—å
        if not pool_path.exists():
            urls_to_download[url] = pool_path

    print(f"üì• –ù—É–∂–Ω–æ —Å–∫–∞—á–∞—Ç—å {len(urls_to_download)} –Ω–æ–≤—ã—Ö –∏–∫–æ–Ω–æ–∫")

    if urls_to_download:
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø–∞–∫–µ—Ç—ã –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        items = list(urls_to_download.items())
        batch_size = 50
        batches = [items[i:i+batch_size] for i in range(0, len(items), batch_size)]
        
        total_downloaded = 0
        start_time = time.time()
        
        with timeout_context(MAX_ICON_DOWNLOAD_TIME):
            with requests.Session() as session:
                adapter = requests.adapters.HTTPAdapter(
                    pool_connections=ICON_DOWNLOAD_WORKERS, 
                    pool_maxsize=ICON_DOWNLOAD_WORKERS,
                    max_retries=1
                )
                session.mount('http://', adapter)
                session.mount('https://', adapter)
                
                with ThreadPoolExecutor(max_workers=ICON_DOWNLOAD_WORKERS) as executor:
                    future_to_batch = {
                        executor.submit(download_icon_batch, session, batch): i 
                        for i, batch in enumerate(batches)
                    }
                    
                    for future in as_completed(future_to_batch, timeout=MAX_ICON_DOWNLOAD_TIME):
                        if timeout_handler.timeout_occurred:
                            break
                        try:
                            batch_idx = future_to_batch[future]
                            successful = future.result(timeout=60)
                            total_downloaded += successful
                            
                            elapsed = time.time() - start_time
                            progress = ((batch_idx + 1) / len(batches)) * 100
                            print(f"   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}% | –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {total_downloaded} | –í—Ä–µ–º—è: {elapsed:.0f}—Å")
                            
                        except TimeoutError:
                            print(f"   ‚ö†Ô∏è  –¢–∞–π–º–∞—É—Ç –ø–∞–∫–µ—Ç–∞ {batch_idx + 1}")
                        except Exception as e:
                            print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞–∫–µ—Ç–∞: {e}")
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∫–æ–Ω–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {total_downloaded} –∏–∑ {len(urls_to_download)}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞—Ä—Ç—É –∏–∫–æ–Ω–æ–∫
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞—Ä—Ç—ã –∏–∫–æ–Ω–æ–∫...")
    try:
        with open(ICONS_MAP_FILE, 'w', encoding='utf-8') as f:
            json.dump(icon_data, f, ensure_ascii=False, indent=2, cls=CustomEncoder)
        print("‚úÖ –ö–∞—Ä—Ç–∞ –∏–∫–æ–Ω–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–∞—Ä—Ç—ã: {e}")
    
    return icon_data

def load_icon_data_for_daily_update():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–∞—Ä—Ç—É –∏–∫–æ–Ω–æ–∫ –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è."""
    print("\n--- –≠—Ç–∞–ø 1: –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–∞—Ä—Ç—ã –∏–∫–æ–Ω–æ–∫ ---")
    if not ICONS_MAP_FILE.is_file():
        print(f"üìÅ –§–∞–π–ª {ICONS_MAP_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò–∫–æ–Ω–∫–∏ –Ω–µ –±—É–¥—É—Ç –∑–∞–º–µ–Ω–µ–Ω—ã.")
        print("üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—Ä—Ç—ã –∏–∫–æ–Ω–æ–∫.")
        return None
    try:
        with open(ICONS_MAP_FILE, 'r', encoding='utf-8') as f:
            icon_data = json.load(f)
        
        if 'icon_pool' in icon_data:
            icon_data['icon_pool'] = {k: Path(v) for k, v in icon_data['icon_pool'].items()}
        
        groups_count = len(icon_data.get('groups', {}))
        pool_count = len(icon_data.get('icon_pool', {}))
        print(f"‚úÖ –ö–∞—Ä—Ç–∞ –∏–∫–æ–Ω–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {groups_count} –≥—Ä—É–ø–ø, {pool_count} –∏–∫–æ–Ω–æ–∫ –≤ –ø—É–ª–µ")
        return icon_data
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {ICONS_MAP_FILE}: {e}")
        return None

def process_epg_file(file_path, group_map, icon_pool, owner, repo_name, entry):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç EPG-—Ñ–∞–π–ª —Å –∑–∞–º–µ–Ω–æ–π URL –∏–∫–æ–Ω–æ–∫."""
    print(f"üîß –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: {entry['desc']}")
    
    if not group_map or not icon_pool:
        print(f"   ‚ö†Ô∏è  –ù–µ—Ç –∫–∞—Ä—Ç—ã –∏–∫–æ–Ω–æ–∫, –ø—Ä–æ–ø—É—Å–∫–∞—é –∑–∞–º–µ–Ω—É")
        return True

    try:
        was_gzipped = is_gzipped(file_path)
        open_func = gzip.open if was_gzipped else open
        parser = etree.XMLParser(remove_blank_text=True, recover=True)
        
        with open_func(file_path, 'rb') as f:
            tree = etree.parse(f, parser)
        root = tree.getroot()
        
        changes_made = 0
        processed_channels = 0
        
        for channel in root.findall('channel'):
            if timeout_handler.timeout_occurred:
                break
                
            processed_channels += 1
            if processed_channels % 1000 == 0:
                print(f"   üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞–Ω–∞–ª–æ–≤: {processed_channels}")
            
            channel_id = channel.get('id')
            icon_url_pointer = group_map.get(channel_id)
            
            if icon_url_pointer:
                matched_icon_path = icon_pool.get(icon_url_pointer)
                
                if matched_icon_path and matched_icon_path.exists():
                    new_icon_url = RAW_BASE_URL.format(
                        owner=owner, 
                        repo=repo_name, 
                        filepath=str(matched_icon_path).replace('\\', '/')
                    )
                    icon_tag = channel.find('icon')
                    if icon_tag is None:
                        icon_tag = etree.SubElement(channel, 'icon')
                    
                    if icon_tag.get('src') != new_icon_url:
                        icon_tag.set('src', new_icon_url)
                        changes_made += 1
        
        if changes_made > 0:
            print(f"   ‚úÖ –í–Ω–µ—Å–µ–Ω–æ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {changes_made}")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –¥–ª—è –∞—Ä—Ö–∏–≤–∞
            original_filename = Path(urlparse(entry['url']).path).name
            if original_filename.lower().endswith('.gz'):
                archive_internal_name = original_filename[:-3]
            else:
                archive_internal_name = f"{original_filename}.xml"
            
            # –°–æ–∑–¥–∞–µ–º XML
            doctype_str = '<!DOCTYPE tv SYSTEM "https://iptvx.one/xmltv.dtd">'
            xml_bytes = etree.tostring(
                tree, 
                pretty_print=True, 
                xml_declaration=True, 
                encoding='UTF-8', 
                doctype=doctype_str
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
            if was_gzipped:
                with gzip.GzipFile(
                    filename=archive_internal_name, 
                    mode='wb', 
                    fileobj=open(file_path, 'wb'), 
                    mtime=0
                ) as f_out:
                    f_out.write(xml_bytes)
            else:
                with open(file_path, 'wb') as f_out:
                    f_out.write(xml_bytes)
        else:
            print(f"   ‚ÑπÔ∏è  –ò–∑–º–µ–Ω–µ–Ω–∏–π –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file_path}: {e}")
        return False

def update_readme(results, notes):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç README.md —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è."""
    timestamp = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M %Z')
    lines = []
    
    if notes:
        lines.extend([notes, "\n---"])
    
    lines.append(f"\n# üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ: {timestamp}\n")
    
    successful = sum(1 for r in results if not r.get('error'))
    failed = len(results) - successful
    
    lines.append(f"üìä **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:** {successful} —É—Å–ø–µ—à–Ω–æ, {failed} –æ—à–∏–±–æ–∫\n")
    
    for idx, r in enumerate(results, 1):
        lines.append(f"**{idx}. {r['entry']['desc']}**\n")
        if r.get('error'):
            lines.extend([
                f"**–°—Ç–∞—Ç—É—Å:** ‚ùå –û—à–∏–±–∫–∞",
                f"**–ò—Å—Ç–æ—á–Ω–∏–∫:** `{r['entry']['url']}`",
                f"**–ü—Ä–∏—á–∏–Ω–∞:** {r.get('error')}",
                "\n---"
            ])
        else:
            lines.extend([
                f"**–†–∞–∑–º–µ—Ä:** {r['size_mb']} MB",
                "",
                "**–°—Å—ã–ª–∫–∞ –¥–ª—è –ø–ª–µ–µ—Ä–∞ (GitHub Raw):**",
                f"`{r['raw_url']}`",
                "\n---"
            ])
    
    try:
        with open(README_FILE, 'w', encoding='utf-8') as f:
            f.write("\n".join(lines))
        print(f"‚úÖ README.md –æ–±–Ω–æ–≤–ª—ë–Ω ({len(results)} –∑–∞–ø–∏—Å–µ–π)")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è README: {e}")

def main():
    print("üöÄ –ó–∞–ø—É—Å–∫ EPG Updater Script")
    
    parser = argparse.ArgumentParser(description="EPG Updater Script")
    parser.add_argument('--full-update', action='store_true', 
                       help='–í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–ª–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–∫–ª—é—á–∞—è –∏–∫–æ–Ω–∫–∏')
    args = parser.parse_args()

    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏
    repo = os.getenv('GITHUB_REPOSITORY')
    if not repo or '/' not in repo:
        sys.exit("‚ùå –û—à–∏–±–∫–∞: –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è GITHUB_REPOSITORY –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞.")
    owner, repo_name = repo.split('/')
    
    print(f"üìÅ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {owner}/{repo_name}")
    
    # –ß–∏—Ç–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    sources, notes = read_sources_and_notes()
    print(f"üìã –ù–∞–π–¥–µ–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(sources)}")
    
    # –≠—Ç–∞–ø 0: –ó–∞–≥—Ä—É–∑–∫–∞ EPG —Ñ–∞–π–ª–æ–≤
    print("\n--- –≠—Ç–∞–ø 0: –ó–∞–≥—Ä—É–∑–∫–∞ EPG —Ñ–∞–π–ª–æ–≤ ---")
    clear_directory(DATA_DIR)
    
    download_results = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_entry = {executor.submit(download_one, entry): entry for entry in sources}
        
        for future in as_completed(future_to_entry):
            if timeout_handler.timeout_occurred:
                print("‚ö†Ô∏è  –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ —Ç–∞–π–º–∞—É—Ç—É")
                break
            try:
                result = future.result(timeout=REQUEST_TIMEOUT * 2)
                download_results.append(result)
            except TimeoutError:
                entry = future_to_entry[future]
                download_results.append({
                    'entry': entry, 
                    'error': '–¢–∞–π–º–∞—É—Ç –∑–∞–≥—Ä—É–∑–∫–∏'
                })

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
    icon_data = None
    if args.full_update:
        print("\nüîÑ –†–µ–∂–∏–º: –ü–û–õ–ù–û–ï –û–ë–ù–û–í–õ–ï–ù–ò–ï")
        clear_directory(ICONS_DIR)
        icon_data = perform_full_update(download_results)
    else:
        print("\nüìÖ –†–µ–∂–∏–º: –ï–ñ–ï–î–ù–ï–í–ù–û–ï –û–ë–ù–û–í–õ–ï–ù–ò–ï")  
        icon_data = load_icon_data_for_daily_update()

    # –≠—Ç–∞–ø 3: –ó–∞–º–µ–Ω–∞ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏–∫–æ–Ω–∫–∏
    print("\n--- –≠—Ç–∞–ø 3: –ó–∞–º–µ–Ω–∞ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏–∫–æ–Ω–∫–∏ ---")
    if icon_data:
        icon_pool = icon_data.get('icon_pool', {})
        processing_futures = []
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            for res in download_results:
                if res.get('error') or timeout_handler.timeout_occurred: 
                    continue
                
                source_url = res['entry']['url']
                group_hash = icon_data['source_to_group'].get(source_url)
                
                group_map = {}
                if group_hash and group_hash in icon_data['groups']:
                    group_map = icon_data['groups'][group_hash].get('icon_map', {})
                
                future = executor.submit(
                    process_epg_file, 
                    res['temp_path'], 
                    group_map, 
                    icon_pool, 
                    owner, 
                    repo_name, 
                    res['entry']
                )
                processing_futures.append(future)
            
            # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            for future in as_completed(processing_futures):
                if timeout_handler.timeout_occurred:
                    break
                try:
                    future.result(timeout=300)
                except TimeoutError:
                    print("‚ö†Ô∏è  –¢–∞–π–º–∞—É—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞")
    else:
        print("‚ÑπÔ∏è  –î–∞–Ω–Ω—ã–µ –æ–± –∏–∫–æ–Ω–∫–∞—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç, –∑–∞–º–µ–Ω–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∞")

    # –≠—Ç–∞–ø 4: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è
    print("\n--- –≠—Ç–∞–ø 4: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è ---")
    
    # –°–æ–∑–¥–∞–µ–º —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    url_to_result = {res['entry']['url']: res for res in download_results}
    ordered_results = [url_to_result[s['url']] for s in sources]
    
    final_results = []
    used_names = set()

    for res in ordered_results:
        if res.get('error'):
            final_results.append(res)
            continue
            
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
        final_filename_from_url = Path(urlparse(res['entry']['url']).path).name
        if not Path(final_filename_from_url).suffix:
            ext = '.xml.gz' if is_gzipped(res['temp_path']) else '.xml'
            proposed_filename = f"{final_filename_from_url}{ext}"
        else:
            proposed_filename = final_filename_from_url

        # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∏–º–µ–Ω
        final_name, counter = proposed_filename, 1
        while final_name in used_names:
            p_stem = Path(proposed_filename).stem
            p_suffixes = "".join(Path(proposed_filename).suffixes)
            final_name = f"{p_stem}-{counter}{p_suffixes}"
            counter += 1
        used_names.add(final_name)
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Ñ–∞–π–ª
        target_path = DATA_DIR / final_name
        try:
            res['temp_path'].rename(target_path)
            res['raw_url'] = RAW_BASE_URL.format(
                owner=owner, 
                repo=repo_name, 
                filepath=str(target_path).replace('\\', '/')
            )
        except Exception as e:
            res['error'] = f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}"
        
        final_results.append(res)

    # –û–±–Ω–æ–≤–ª—è–µ–º README
    update_readme(final_results, notes)
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    successful = sum(1 for r in final_results if not r.get('error'))
    failed = len(final_results) - successful
    
    print(f"\nüéâ –°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç: {successful} —É—Å–ø–µ—à–Ω–æ, {failed} –æ—à–∏–±–æ–∫")
    
    if timeout_handler.timeout_occurred:
        print("‚ö†Ô∏è  –í–Ω–∏–º–∞–Ω–∏–µ: –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –±—ã–ª–æ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ —Ç–∞–π–º–∞—É—Ç—É")
        sys.exit(1)

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\nüõë –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        print(f"\nüí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)
